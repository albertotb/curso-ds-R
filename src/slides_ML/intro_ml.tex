\documentclass{beamer}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{fancyvrb}
\usepackage{listings}
\usepackage{eurosym}
\usepackage{xcolor}
\usepackage{textcomp}
\usepackage[ruled, lined]{algorithm2e}
\usetheme{simple}

\let\Tiny=\tiny

\newcommand{\mytilde}{$\sim$}

\definecolor{links}{HTML}{2A1B81}
\hypersetup{colorlinks,linkcolor=,urlcolor=links}

\addtolength{\jot}{0.6em}  % extra space between align rows
\DeclareUnicodeCharacter{20AC}{\euro}

\newcommand{\Xbf}{\ensuremath{\mathbf{X}}}
\newcommand{\xbf}{\ensuremath{\mathbf{x}}}
\newcommand{\wbf}{\ensuremath{\mathbf{w}}}
\newcommand{\ybf}{\ensuremath{\mathbf{y}}}
\newcommand{\epbf}{\ensuremath{\boldsymbol{\epsilon}}}
\newcommand{\Rbb}{\ensuremath{\mathbb{R}}}
\newcommand{\Ebb}{\ensuremath{\mathbb{E}}}

\DeclareMathOperator*{\argmin}{arg\,min}

\usepackage{tikz}
\usetikzlibrary{arrows,shapes,positioning}

\usepackage{multirow}
\usepackage{booktabs}

\graphicspath{{./img/}}

\newenvironment{wideitemize}{\itemize\addtolength{\itemsep}{12pt}}{\enditemize}

\title{Introducción al Aprendizaje Automático}
\author{Alberto Torres Barrán}
\date{3 de Enero del 2020}

%\usepackage{Sweave}
\begin{document}

%% Titulo
\begin{frame}[plain]
\titlepage
\end{frame}

\begin{frame}
	\frametitle{Índice}
    \tableofcontents
\end{frame}

\AtBeginSection[]
{
   \begin{frame}
       \frametitle{Índice}
       \tableofcontents[currentsection]
   \end{frame}
}

\section{Introducción}

\begin{frame}
\frametitle{Tipos de aprendizaje}

\begin{wideitemize}
\item \textbf{Supervisado}: dados pares de entrada-salida, el objetivo es inferir su relación \\
Ejemplos: regresión linea, regresión logística 

\item \textbf{No supervisado}: dados unos datos de entrada, el objetivo es inferir cierta estructura inherente en los mismos, sin necesidad de especificar las salidas de forma explícita\\
Ejemplos: clustering, reducción de dimensión

\item Existen otros tipos de tareas en los que el acceso a las salidas está limitado de distintas formas:
\begin{itemize}
\item Aprendizaje activo
\item Aprendizaje semi-supervisado
\item Aprendizaje por refuerzo
\end{itemize}
\end{wideitemize}
\end{frame}


\begin{frame}
\frametitle{Aprendizaje supervisado}

\begin{itemize}
\item Dado un conjunto de datos, compuesto por observaciones de diferentes variables, llamamos \textbf{variable respuesta} a aquella que es objeto del estudio.

\item Una vez identificada la respuesta, tenemos dos objetivos principales:

\begin{itemize}
\item Predicción: ser capaz de predecir cual va a ser la respuesta para observaciones futuras.
\item Información: extraer información sobre la relación de la variable de respuesta con el resto.
\end{itemize}

\item A su vez distinguimos dos tipos de problemas:

\begin{itemize}
\item Regresión: la variable respuesta es continua.
\item Clasificación: la variable respuesta es discreta (número finito de categorias).
\end{itemize}
\end{itemize}
\end{frame}


\section{Preproceso de datos}

\begin{frame}
\frametitle{Primeros pasos}

\begin{itemize}
\item Los datos a analizar a menudo provienen de fuentes variadas (redes sociales, sensores, encuestas, ...) y están almacenados en diferentes soportes (ficheros de texto, base de datos, ficheros binarios, \textit{streams}...).
\item Lo primero es identificar el problema qué queremos resolver y cuales son las variables que tenemos disponibles y pueden aportar información.
\item Ante la duda, no descartar variables/información ni observaciones antes de tiempo.
\item Lo segundo es combinar todas esa información y transformarla en una mezcla de variables numéricas (valores continuos) y categóricas (valores discretos).
\item El objetivo final del preproceso es organizar esos datos en un formato tabular (filas y columnas).
\end{itemize}

\end{frame}

\begin{frame}
\frametitle{Distintos tipos de información}

\begin{itemize}\addtolength{\itemsep}{\fill}
\item En ocasiones no es trivial transformar ciertos tipos de información en variables numéricas y/o categóricas.
\item Para estos casos a menudo es necesario un preproceso extra, muy dependiente del problema a resolver y específico del dominio.
\item Ejemplos típicos:
\begin{itemize}
\item Texto (tweets, páginas web, documentos): \textit{word2vec}, \textit{bag-of-words}, modelos \textit{n-gram}.
\item Imágenes: valores RGB de los píxeles, intensidad de gris.
\item Audio: transformada de Fourier, coeficientes MFCC.
\item Video: secuencia de \textit{frames}.
\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}[allowframebreaks]
\frametitle{Ejemplo: MNIST}
\begin{center}
\includegraphics[width=0.85\textwidth]{mnist.png}

{\footnotesize \href{http://yann.lecun.com/exdb/mnist/}{Base de datos MNIST}}
\end{center}

Cada dígito:
\begin{itemize}
\item se recorta
\item se escala para que ocupe exactamente $20 \times 20$ píxeles.
\item se centra en un recuadro de $28 \times 28$ píxeles.
\end{itemize}
Para transformarlo en datos tabulares:
\begin{enumerate}
\item consideramos cada imágen como una matriz de $28$ filas y $28$ columnas donde se almacenan los valores de 0 (negro) a 255 (blanco) de intensidad de gris.
\item convertimos esa matriz un un vector muy largo de $28\times 28=784$ elementos.
\item apilamos los vectores de los distintos números uno encima de otro.
\end{enumerate}
\end{frame}
%
%
%\begin{frame}
%\frametitle{\textit{Tidy data}}
%
%\begin{itemize} \setlength{\itemsep}{\fill}
%\item El 80\% del tiempo del análisis de datos se pasa limpiando y preparando datos (Dasu and Johnson 2003).
%
%\item Una vez cargados los datos, es conveniente estructurarlos de forma que el procesado posterior sea lo más sencillo posible.
%
%\item Una estructura muy común son los datos ordenados o \textit{tidy data}.
%
%\item Hadley Wickham (\href{http://vita.had.co.nz/papers/tidy-data.pdf}{2014}) los define como aquellos donde:
%
%\begin{enumerate}
%\item Cada variable forma una columna.
%\item Cada observación o muestra forma una fila.
%\item Cada tipo de unidad de observación forma una tabla.
%\end{enumerate}
%\end{itemize}
%\end{frame}


\begin{frame}
\frametitle{\textit{Missing values}}

\begin{itemize}
\item Es importante distinguir cuando una variable tiene valor $0$ ó no conocido.
\item Estos valores pueden venir representados por múltiples caracteres (``*", ``-", campo vacio, etc.).
\item Hay que codificarlos de manera especial para tenerlos en cuenta en los análisis.
\item En general,
\begin{enumerate}
\item Si tenemos suficientes datos, podemos simplemente ignorar las observaciones en las que falte alguno.
\item Sino, podemos completar dichas observaciones que faltan con, por ejemplo, la mediana del resto.
\end{enumerate}
\item \textbf{Ejemplo}: en datos que provienen de un reconocimiento médico varios pacientes no tienen ningún valor en el campo de ``Fármacos''. No toman ninguna medicación o el médico no ha registrado la respuesa?
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{\textit{Outliers}}
\begin{wideitemize}
\item Distinguir si un valor es erróneo o válido pero extremo es muy complicado y dependiente del dominio.
\item Existen test estadísticos que permiten identificarlos.
\item En general no son perjudiciales, pero si es importante corregir los valores que son \textbf{imposibles}.
\item \textbf{Ejemplo}: en datos provenientes de un reconocimiento médico, aparece un paciente con un IMC de 50.
\end{wideitemize}
\end{frame}


\begin{frame}
\frametitle{Normalización}

\begin{itemize}
\item Las variables numéricas suelen tener rangos muy diversos.
\item Ejemplo: salario (1000 -- 1000000 \euro) y edad (0--100).
\item Algunos modelos interpretan esta diferencia de escalas como que unas variables son más importantes que otras.
\item Existen varias normalizaciones para que estas variables sean comparables:
\begin{itemize}
\item Media $0$ varianza $1$.
\item Escalar al intervalo $-1$, $1$.
\item ...
\end{itemize}
\item En ocasiones normalizar las variables también puede ayudar a que el proceso de aprendizaje sea más rápido.
\item Cuidado al analizar los resultados, ya que están en los nuevos rangos.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Variables categóricas}

\begin{itemize}
\item Muy comunes en distintas fuentes de datos.
\item Importante tenerlas en cuenta para la fase de preproceso y análisis previo.
\item Muy pocos algoritmos son capaces de tratarlas directamente.
\item Por tanto, antes de pasar a la siguiente etapa (modelado) queremos convertirlas en numéricas.
\item La transformación donde se asigna a cada uno de sus valores un número entero no suele ser buena idea, ya que crea una relación artificial de orden y falsea las distancias.
\item Una forma es utilizar una codificación ``\textit{dummy}''.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Ejemplo codificación ``\textit{dummy}''}

\begin{tabular}{cc}
\toprule
Edad & Sexo \\
\midrule
34 & H \\
18 & M \\
67 & M \\
21 & M \\
15 & H \\
\bottomrule
\end{tabular}
%
\hfill $\boldsymbol{\Longrightarrow}$ \hfill
%
\begin{tabular}{ccc}
\toprule
Edad & Es mujer? & Es hombre?\\
\midrule
34 & 0 & 1 \\
18 & 1 & 0 \\
67 & 1 & 0 \\
21 & 1 & 0 \\
15 & 0 & 1 \\
\bottomrule
\end{tabular}
\hspace*{0.4em}

\vfill
\begin{itemize}
\item Finalmente, podemos eliminar una de las dos nuevas variables puesto que están perfectamente correladas.

\item En general, para una variable categórica con $p$ valores añadimos $p-1$ variables nuevas.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Otras codificaciones}
\begin{itemize}\setlength{\itemsep}{\fill}
\item Si en la semántica de la variable hay implícita una relación de orden: Puntuación \{baja, media, alta\} $\Rightarrow$ \{1,2,3\}.

\item Si hay relación de orden y no queremos falsear las distancias:\\
\vspace{1em}

\begin{tabular}{lrc}
\toprule
Mes & Día & Temp. \\
\midrule
Enero & 29 & 22.2 \\
Enero & 30 & 27.8 \\
Enero & 31 & 28.6 \\
Febrero &  1 & 26.1 \\
Febrero &  2 & 25.3 \\
\bottomrule
\end{tabular}
%
\hfill $\boldsymbol{\Longrightarrow}$ \hfill
%
\begin{tabular}{cc}
\toprule
Días desde 01/01 & Temp. \\
\midrule
29 & 22.2 \\
30 & 27.8 \\
31 & 28.6 \\
32 & 26.1 \\
33 & 25.3 \\
\bottomrule
\end{tabular}
\end{itemize}
\end{frame}


\begin{frame}[fragile]
\frametitle{Factores}
\begin{itemize}
\item Un factor es un tipo de dato que se utiliza para codificar valores categóricos, por ejemplo: \texttt{renta} = \{alta, baja, media\}.
\item Se crean con la función factor:
\begin{verbatim}
   > f <- factor(c("hombre", "mujer", "mujer"))
\end{verbatim}
\item Se pueden ver los niveles (valores distintos) con la función \texttt{levels()}:
\begin{verbatim}
   > levels(f)
   [1] "hombre" "mujer"
\end{verbatim}
\item Función \texttt{relevel()}: reordena los niveles del factor especificado, poniendo el nivel especificado de primero
\end{itemize}
\end{frame}

%\begin{frame}
%  \frametitle{Operaciones con factores}
%  \begin{itemize}
%    \item Función \texttt{cut()}: crea un factor dividiendo en rangos un vector numérico de acuerdo a unos puntos de corte
%    \item Función \texttt{tapply()}: aplica una función a cada uno de los elementos de un vector, dividos en los distintos grupos de un determinado factor.
%    \item Función \texttt{by()}: similar a la anterior, pero el objeto sobre el que se aplica la operación agrupada puede ser un \texttt{data.frame}.
%    \item Función \texttt{aggregate()}: similar a \texttt{tapply()} pero devuelve un \texttt{data.frame} y acepta ``fórmulas''.
%    \item Funciones \texttt{table()}, \texttt{prop.table()}, \texttt{margin.table()} y \texttt{xtabs()}: crear tablas de contingencia a partir de ciertos factores
%  \end{itemize}
%\end{frame}


\begin{frame}
\frametitle{Visualización}

\begin{itemize}
\item En ocasiones es útil hacer gráficos de algunas variables para ver que tipo de relación tienen con la respuesta.
\item Sin embargo, a medida que los conjuntos de datos son más grandes:
\begin{enumerate}
\item el número de variables puede ser muy grande, incluso del orden de millones.
\item la relación de las variables de entrada es muy compleja y altamente no lineal.
\end{enumerate}
\item En esos casos es muy difícil hacer gráficos que proporcionen información relevante, ya que podemos representar como mucho 2 o 3 dimensiones.
\item Los gráficos múltiples (facetas) y algunas transformaciones estadísticas (reducción de dimensionalidad) pueden aliviar el problema.
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Ejemplo}

\begin{center}
\includegraphics[width=\textwidth]{PA.png}
\end{center}

\end{frame}


\section{Modelos lineales básicos}

\subsection{Regresión lineal}

\begin{frame}
\frametitle{Regresión lineal: ejemplo}

\centering
\includegraphics[width=0.7\textwidth]{linear_regression.pdf}
\end{frame}


\begin{frame}
\frametitle{Regresión lineal}

\begin{wideitemize}

\item La regresión linear asume que la variable de respuesta $y$ depende linealmente de las variables independientes,

$$y = w_0 + x_1w_1 + x_2w_2 + x_3w_3 + \dots + x_dw_d$$

\item Si tenemos $n$ observaciones de cada una de las variables, podemos escribirlo en notación matricial

$$\ybf = \Xbf\wbf$$

\item El término de bias $w_0$ se incluye como una columna constante de $1$s en $\Xbf$

\item El objetivo es estimar los pesos o coeficientes $w$
\end{wideitemize}
\end{frame}


\begin{frame}
\frametitle{Hipótesis de la regresión lineal}

\begin{wideitemize}
\item El método más común para calcular la recta de regresión se conoce como mínimos cuadrados.

%\item Consiste en una ``simple'' fórmula, y se puede aplicar para cualquier número de variables.

\item Teóricamente, para que el ajuste esté bien definido se asume que:
\begin{enumerate}
\item la respuesta depende linealmente de las variables
\item el modelo está especificado correctamente (no faltan variables)
\item hay menos variables que observaciones
\item no hay dos variables con correlación perfecta
\end{enumerate}

\item Es un modelo preditivo, ya que nos permite calcular el valor de $y$ para nuevos valores de $x$.

\end{wideitemize}
\end{frame}


\begin{frame}
\frametitle{Mínimos cuadrados}

\centering
\includegraphics[width=\textwidth]{minimos_cuadrados.jpg}

\end{frame}



\begin{frame}
\frametitle{Cuarteto de Anscombe}

La pregunta es, ¿cómo de bien se ajusta la recta a nuestros datos?

\vfill
\centering
\hspace*{-1.5em}
\includegraphics[width=0.8\textwidth]{anscombe.pdf}
\end{frame}


\begin{frame}
\frametitle{Bondad de ajuste}

\begin{wideitemize}
\item Históricamente se medía la calidad del modelo ó ``bondad del ajuste'' con diversos test sobre los residuos:
\begin{itemize}
\item Homocedásticos (varianza constante)
\item Media cero
\item Sin autocorrelación
\item (Distribución normal)
\end{itemize}

%\item Ya en 2001 Breiman cuestionaba si examinando los residuos se podía medir la aplicabilidad del modelo en cierta clase de problemas.

\item El módelo puede ser útil a pesar de que las hipótesis de la regresión y los test sobre los residuos no se cumplan.

\item A menudo nos interesa únicamente la \textbf{capacidad predictiva}.

\item ``\textit{All models are wrong, but some are useful}'' (George Box).
\end{wideitemize}
\end{frame}

\begin{frame}
\frametitle{Minimización del riesgo empírico}

\begin{itemize}
\item Necesitamos definir formalmente a que nos referimos con \textbf{capacidad predictiva}.

\item Dadas unas variables $\xbf$, queremos encontrar una función $f(\xbf)$ que se parezca lo máximo posible a la respuesta $y$.

\item Para ello, necesitamos una función de pérdida $L(f(\xbf), y)$ que cuantifique cuando de diferente es nuestra predicción del valor real.

\item El problema de aprendizaje consiste por tanto en encontrar la función $f$ que minimiza la pérdida media de todas las observaciones: $$\hat{f} = \argmin\; \frac{1}{n}\sum_{i=1}^{n}{L(f(\xbf_i), y_i)}$$
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Sobreajuste}

\begin{itemize}
\item Si resolvemos el problema anterior con el error cuadrático como función de pérdida $L(f(\xbf), y) = (f(\xbf)-y)^2$, obtenemos el estimador de mínimos cuadrados.

\item Por tanto, ya tenemos una forma de calcular el error de predicción de nuestro modelo original.

\item La pregunta ahora es, podemos mejorar el modelo (disminuir su error)?

\item Sí, aunque el modelo tiene que ser lineal en las variables, se pueden añadir nuevas variables que sean transformaciones polinómicas.

\item De hecho, siempre podemos añadir expansiones polinómicas de forma que el error sea casi $0$.
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Sobreajuste: ejemplo}
\centering
\includegraphics[height=0.85\textheight]{overfit1.pdf}
\end{frame}

\begin{frame}
\frametitle{Equilibrio sesgo-varianza: intuición}
\begin{itemize}
\item Si el modelo es muy simple, la solución esta sesgada y no ajusta bien los datos.
\item Si el modelo es muy complejo, es muy sensible a pequeños cambios en los datos.

\item En general el error calculado sobre las muestras usadas para entrenar el modelo (error de \textbf{entrenamiento}) es demasiado optimista.

\item El error de entrenamiento se puede hacer arbitrariamente pequeño aumentando la complejidad del modelo.

\item Nos interesa el error de \textbf{generalización}, es decir el error sobre muestras que el modelo no conoce.
\end{itemize}

\end{frame}

\begin{frame}
\frametitle{Equilibrio sesgo-variance: definición gráfica}
\centering
\includegraphics[height=0.80\textheight]{bias_variance_dardos.jpg}

{\footnotesize \href{http://blog.fliptop.com/blog/2015/03/02/bias-variance-and-overfitting-machine-learning-overview/}{Fuente}}
\end{frame}


\begin{frame}
\frametitle{Equilibrio sesgo-varianza: formulación}

El error teórico de predicción es
%
\begin{equation*}
\text{EP} = \Ebb[(y - \hat{f}(x))^2].
\end{equation*}
%
Se puede descomponer en
%
\begin{align*}
\text{EP} &= \underbrace{\Bigl(\Ebb[\hat{f}(x)] - f(x) \Bigr)^2}_{\text{Sesgo}^2}
           + \underbrace{E\Bigl[\hat{f}(x) - \Ebb[\hat{f}(x)] \Bigr]^2}_{\text{Varianza}}
           + \underbrace{\vphantom{\Bigl(}\sigma^2}_{\text{Ruido}}
\end{align*}
%
\begin{itemize}
\item Los términos de sesgo y varianza son opuestos: si disminuimos uno el otro aumenta y viceversa.
\item El término del ruido es inherente a los datos y no podemos hacer nada.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Conjuntos de entrenamiento y test}

\begin{itemize}
\item En la práctica, lo primero que hacemos cuando cargamos unos datos es dividirlos aleatoriamente en dos subconjuntos, entrenamiento y test.
\item El subconjunto de test nos lo guardamos y no se utiliza \textbf{nunca} en la fase de aprendizaje del modelo.
\item Una vez construido el modelo, se comprueba su rendimiento en el conjunto de test.
\item Este último error es una buena estimación no sesgada de como se va a comportar nuestro modelo con nuevos datos.
\item Existe una gran probabilidad de sobreajuste si el error de test es muy alto en comparación con el error de entrenamiento.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Error de predicción en función de la complejidad}
\centering
\includegraphics[height=0.75\textheight]{biasvariance.pdf}
\end{frame}


\begin{frame}
\frametitle{Errores de regresión}

Dado el valor real de la observación $i$, $y_i$ y la predicción del modelo $\hat{y}_i$, podemos calcular:

\begin{itemize}
\item MAE (Mean absolute error)

$$\text{MAE} = \frac{1}{n}\sum_{i=1}^{n}{|y_i - \hat{y}_i|}$$

\item MSE (Mean squared error)

$$\text{MSE} = \frac{1}{n}\sum_{i=1}^{n}{(y_i - \hat{y}_i)^2}$$

\end{itemize}
\end{frame}



\begin{frame}[fragile]
\frametitle{Modelos lineales en R}
\begin{itemize}
\item En R un modelo lineal de la variable de respuesta $y$ sobre las variables $x_1$, \dots, $x_d$, se define con la fórmula $$y \sim x_1 + x_2 + \dots + x_d$$
\item Para ajustar un modelo lineal por mínimos cuadrados se usa la función \texttt{lm()}, pasando como primer argumento la fórmula anterior.
\item Las fórmulas también pueden contener expresiones aritméticas de variables (expansiones polinómicas, logaritmos, etc).
\item Ejemplo: modelo de regresión de la variable \texttt{mpg} sobre \texttt{wt}

\begin{Verbatim}[commandchars=\\\{\}]
   > fit <- lm(mpg \mytilde wt, data=mtcars)
\end{Verbatim}
\end{itemize}
\end{frame}


\begin{frame}[fragile]
\frametitle{Fórmulas}
\begin{itemize}
\item Las fórmulas son objetos especiales de R que respresentan relaciones simbólicas entre variables:

\texttt{respuesta \mytilde\  variables independientes}

\item Se usan en funciones como \texttt{aggregate()}, \texttt{boxplot()}, y \texttt{lm()}.

\item Los operadores aritméticos tienen otro significado cuando se usan dentro de las fórmulas. Ejemplos:

\begin{Verbatim}[commandchars=\\\{\}]
   y \mytilde u + v + w + u:v + u:w + v:w
   y \mytilde u * v * w - u:v:w
   y \mytilde (u + v + w)^2
\end{Verbatim}

\item Si queremos que tengan su significado habitual tenemos que utilizar el operador \texttt{I()}:
\begin{Verbatim}[commandchars=\\\{\}]
   y \mytilde u + v + w + I(u*v) + I(u*w) + I(v*w)
\end{Verbatim}
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Ejercicio regresión}
Con el conjunto de datos \texttt{diamonds}:
\begin{itemize}
\item Separarlos aleatoriamente en un 60\% de entrenamiento y un 40\% de test.
\item Ajustar un modelo lineal del precio sobre los quilates.
\item Ajustar un modelo cuadrático con las mismas variables.
\item Calcular el error cuadrático medio sobre el conjunto de entrenamiento y test, definido como $$\text{ECM} = \frac{1}{n}\sum_{i=1}^{n}{(y_i - \hat{y}_i)^2},$$ donde $y$ es la variable respuesta y $\hat{y}$ la predicción del modelo.
\item ¿Qué pasa ahora si ajustamos el modelo sobre todas las variables? ¿Disminuye el error?
\end{itemize}
\end{frame}


\subsection{Regresión logística}

\begin{frame}
\frametitle{Regresión logística}

\begin{wideitemize}
\item Es un modelo lineal para problemas de clasificación.
\item En lugar de una variable continua, la respuesta es ahora una variable discreta con $2$ o más valores, que se denominan \textit{clases}.
\item En el caso binario, el modelo estima la probabilidad de que cada uno de los ejemplos pertenezca a la clase $0$ o $1$.
\item Finalmente se predice la clase $0$ si la probabilidad es menor que $0.5$ y la clase $1$ en caso contrario.
\item Se puede ver como una caso especial del modelo lineal generalizado.
\end{wideitemize}
\end{frame}

\begin{frame}
\frametitle{Regresión logística: formulación}

\begin{wideitemize}
\item La fórmula de la regresión logística es
$$y = \sigma(w_0 + w_1x_1 + w_2x_2 + ... + w_dx_d)$$
donde $\sigma(\cdot)$ es la función logística
$$\sigma(t) = \frac{1}{1 + e^{-t}}$$

\item La función logística siempre tiene como salida un número en el intervalo $(0, 1)$

\item Por tanto, interpretamos la salida del modelo como la probabilidad de pertenecer a una clase o a la otra (clasificación binaria)

\end{wideitemize}
\end{frame}


\begin{frame}
\frametitle{Regresión logística: ejemplo}
\centering
\includegraphics[height=0.85\textheight]{log_reg.png}
\end{frame}


\begin{frame}
\frametitle{Interpretación de coeficientes: odds ratio}
\begin{itemize}
\item Dado un modelo lineal con una única variable independiente, el odds ratio se define como

$$\text{OR} = \frac{\exp(w_0 + w_1(x+1))}{\exp(w_0 + w_1x)} = \exp(w_1)$$

\item Es decir, como cambian las probabilidades de la salida cuando la variable $x$ aumenta una unidad:
\begin{itemize}
\item Si $\text{OR}=1$ la variable $x$ no tiene ninguna asociación con la salida
\item Si $\text{OR}>1$ la variable está asociada con una mayor probabilidad de la salida
\item Si $\text{OR}<1$ la variable está asociada con una menor probabilidad de la salida
\end{itemize}
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Errores de clasificación}

Los principales errores de clasificación se pueden calcular a partir de la \textbf{matriz de confusión}:
\vfill
\begin{columns}[T] % align columns
\begin{column}{.40\textwidth}
\includegraphics[scale=0.5]{confusion_matrix.png}
\end{column}%
\hfill%
\begin{column}{.56\textwidth}
\begin{wideitemize}
\item[] Accuracy: $\frac{\text{TP} + \text{TN}}{\text{P} + \text{N}}$

\item[] Sensitivity, recall, TPR: $\frac{\text{TP}}{\text{TP} + \text{FN}}$

\item[] Specificity, TNR: $\frac{\text{TN}}{\text{TN} + \text{FP}}$

\item[] Precision, PPV: $\frac{\text{TP}}{\text{TP} + \text{FP}}$

\item[] F1 score: $2\times \frac{\text{PPV} \times \text{TPR}}{\text{PPV} + \text{TPR}}$
\end{wideitemize}
\end{column}%
\end{columns}
\end{frame}


\section{Extensiones}

\subsection{Modelos lineales generalizados (GLM)}

\begin{frame}
\frametitle{Modelos lineales generalizados (GLMs)}

\begin{wideitemize}
\item Generalización de la regresión lineal que permite distribuciones de errores distintas de la distribución normal.

\item Se asume que la media de dicha distribución depende de las variables independientes de la siguiente forma:
$$\Ebb(\ybf) = \mu = g^{-1}(\Xbf\wbf)$$
donde $\Ebb(\cdot)$ es el valor esperado y $g$ es la función de enlace

\item La función de enlace proporciona la relación entre la media de la distribución y el predictor lineal
\end{wideitemize}
\end{frame}


\begin{frame}
\frametitle{Ejemplo: distribución de Bernoulli}

\begin{wideitemize}
\item Cuando la distribución de la salida $\ybf$ es una Bernoulli el modelo se conoce con el nombre de regresión logística

\item La función de media es la logística,

$$\mu = g^{-1}(\Xbf\wbf) = \frac{1}{1 + \exp(-\Xbf\wbf)}$$

\item La función de enlace es la inversa de la anterior,

$$\Xbf\wbf = g(\mu) = \ln\left(\frac{\mu}{1 - \mu}\right)$$

\item Para cada distribución, hay una función de enlace ``canónica'' que es la que se usa habitualmente
\end{wideitemize}

\end{frame}


\begin{frame}
\frametitle{Ejemplo: distribución de Poisson}

\begin{wideitemize}
\item Esta distribución está indicada cuando queremos modelizar una variable de salida entera y no real (por ej. conteos)

\item Función de media

$$\mu = \exp(\Xbf\wbf)$$

\item Función de enlace

$$\Xbf\wbf = \ln(\mu)$$

\item Otras distribuciones posibles son la Gamma, Exponencial, Multinomial, etc.
\end{wideitemize}
\end{frame}


\begin{frame}[fragile]
\frametitle{GLMs en R}
\begin{wideitemize}
\item La función para ajustar modelos lineales generalizados es \texttt{glm()}
\item Tiene los mismos argumentos principales que \texttt{lm()}, pero además tenemos que especificar la distribución de la variables dependiente con el parámetro \texttt{family}
\item Ejemplo: regresión logística

\begin{Verbatim}[commandchars=\\\{\}]
   > fit <- glm(Species \mytilde Petal.Length, data=iris, 
                family=binomial)
\end{Verbatim}
\item Por defecto se usa la función de enlace ``canónica'', pero esto se puede modificar (ver ayuda)
\end{wideitemize}
\end{frame}


\subsection{Regularización}

\begin{frame}
\frametitle{Regularización}

\begin{itemize}
\item El estimador de mínimos cuadrados para la regresión lineal es el \textit{mejor} estimador no sesgado, donde \textit{mejor} se refiere al que tiene menor varianza.

\item Sin embargo, a menudo se puede reducir esta varianza bastante, en detrimento de introducir un pequeño de sesgo.

\item Esto se consigue limitando la complejidad del modelo con un término de \textbf{regularización}.

\item Un ejemplo muy común es Ridge Regression, que añade una regularización $l_2$ a mínimos cuadrados:
$$\min_\wbf\;||\Xbf\wbf - \ybf||^2_2 + \lambda||\wbf||^2_2$$

\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Lasso}

\begin{itemize}
\item Similar a Ridge Regression, Lasso añade un término de regularización $l_1$:

$$\min_\wbf\;||\Xbf\wbf - \ybf||^2_2 + \lambda||\wbf||_1$$

donde 
$$||\wbf||_1 = \sum_{i=1}^{d}{|w_i|}$$

\item El valor absoluto promueve que muchos coeficientes sean $0$ después de ajustar el modelo

\item Dichos coeficientes no tienen por tanto efecto en la salida

\item Se podría considerar que son variables ``poco importantes''
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Elastic Net}

\begin{itemize}
\item Combina las regularizaciones de Ridge y Lasso:
$$\min_\wbf\;||\Xbf\wbf - \ybf||^2_2 + \lambda_1||\wbf||_1 + \lambda_2||\wbf||_2^2$$

\item Otra forma de escribirlo:

$$\min_\wbf\;||\Xbf\wbf - \ybf||^2_2 + \lambda(\alpha||\wbf||_1 + (1 - \alpha)||\wbf||_2^2)$$

\item Para $\alpha = 0$ recuperamos Ridge Regression y para $\alpha = 1$ el Lasso

\item Mantiene la dispersión en los coeficientes del Lasso pero en general se obtienen mejores modelos en términos de error

\item Problema: tenemos que decidir el valor de dos hiper-parámetros ($\alpha$ y $\lambda$)
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Paquete \texttt{glmnet} y \texttt{glmnetUtils}}

\begin{wideitemize}
\item Implementa GLMs con regularización Lasso y Elastic Net

\item Muy eficiente (escrito en Fortran)

\item Se pueden ajustar no solo regresiones lineales con regularización, sino también regresiones logísticas, regresiones de Poisson, etc.

\item Implementa también un mecanismo para seleccionar automáticamente el parámetro $\lambda$ (pero no $\alpha$) usando validación cruzada

\item \texttt{glmnet} no tiene interfaz para fórmulas, pero \texttt{glmnetUtils} incorpora una

%\item Para más información cargar el paquete y escribir \texttt{?glmnet}
\end{wideitemize}
\end{frame}


\begin{frame}[allowframebreaks]
\frametitle{Ejercicio titanic}

Vamos a intentar predecir la supervivencia de las víctimas del Titanic a partir de las siguientes variables:

\begin{itemize}
\item \texttt{survival}:  Supervivencia (0 = No; 1 = Si)
\item \texttt{pclass}:   Clase de pasajero (1, 2, 3)
\item \texttt{name}:      Nombre
\item \texttt{sex}:           Sexo
\item \texttt{age}:          Edad
\item \texttt{sibsp}:           Número de hermanos/esposos/as a bordo.
\item \texttt{parch}:          Número de padres/hijos a bordo
\item \texttt{ticket}:          Número de ticket
\item \texttt{fare}:         Coste del billete
\item \texttt{cabin}:          Cabina
\item \texttt{embarked}:        Puerto de embarque
\end{itemize}

\break

\begin{enumerate}
\item Cargar el fichero \texttt{titanic.csv} en R.
\item Ver cuantos valores \textit{missing} tiene cada variable con \texttt{summary}.
\item Eliminar la variable \texttt{Cabin} (¿por qué?).
\item Eliminar también las variables \texttt{PassengerId}, \texttt{Name} y \texttt{Ticket} (¿por qué?).
\item Eliminar las filas que contengan algún \texttt{NA} (función \texttt{na.omit}).
\item Convertir la variable \texttt{Survived} a un factor.
\item Dividir datos en 80\% \texttt{entrenamiento} y 20\% \texttt{test}, aleatoriamente.
\item Ajustar un modelo de regresión logística estándar y otro con regularización (función \texttt{glm} y paquete \texttt{glmnet}).
\end{enumerate}
\end{frame}

%\subsection{Modelos aditivos generalizados (GAM)}
%\begin{frame}
%\frametitle{Modelos aditivos generalizados (GAM)}
%
%\begin{itemize}
%\item Extensión de los GLMs donde la salida depende linealmente de funciones de las variables predictoras:
%%
%$$\Ebb(y) = g^{-1}(f_1(x_1) + f_2(x_2) + f_3(x_3) + ... + f_d(x_d))$$
%
%\item Las funciones pueden ser distintas para cada una de las variables
%
%\item Pueden ser paramétricas o funciones generales ``suaves''
%
%\item La mayoría de implementaciones modernas restringen las funciones para que sean de la forma
%
%$$f_i(x_i) = \sum_{k=1}^{K_i}\beta_{ik}b_{ik}(x_i)$$
%
%donde $b_{ik}(x_i)$ son funciones de base
%
%\end{itemize}
%\end{frame}
%
%
%\begin{frame}[fragile]
%\frametitle{Paquete \texttt{mgcv}}
%
%\begin{wideitemize}
%\item Implementación de GAMs en R, alternativa al clásico paquete \texttt{gam}
%
%\item Varias funciones de suavizado posible
%
%\item Incluye el grado de ``suavizado'' en el ajuste del modelo (no es necesario especificarlo)
%
%\item Incluye múltiples distribuciones (Binomial, Bernoulli, Poisson, etc.)
%
%\item Ejemplo:
%\begin{Verbatim}[commandchars=\\\{\}]
%   > fit <- gam(mpg \mytilde s(wt), data=mtcars)
%\end{Verbatim}
%
%\end{wideitemize}
%
%\end{frame}
%
%
%\begin{frame}
%\frametitle{Ejercicio}
%Con los datos de diamantes del ejercicio anterior:
%\begin{itemize}
%\item Ajustar un GAM del precio sonbre los quilates
%\item Comparar el error sobre el conjunto de test con el modelo lineal y con el modelo lineal añadiendo una nueva variable que sea el cuadrado del precio
%
%\item Ajustar un modelo Lasso modelizando el precio usando el resto de variables de entrada
%\item ¿Cuál es el valor del lambda óptimo?
%\item ¿Qué variables son las más importantes y cuales son innecesarias?
%\end{itemize}
%\end{frame}

\begin{frame}
\frametitle{Vecinos próximos}

\begin{itemize}
\item Es uno de los algoritmos más sencillos de clasificación.
\item Dado un conjunto de datos, simplemente clasifica nuevas observaciones como la clase más frecuente entre las $k$ observaciones más cercanas.
\item La métrica de distancia más común para calcular la cercanía de las observaciones es la distancia Euclídea:
$$\text{d}(\mathbf{p}, \mathbf{q}) = \sqrt{\sum_{i=1}^{n}{(q_i-p_i)^2}}$$
\item El valor de $k$ es un parámetro del modelo, y puede tener mucha influencia en el rendimiento.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Ventajas y desventajas}
Ventajas
\begin{itemize}
\item Es un modelo muy simple, de hecho no se construye ningún modelo explicitamente.
\item Robusto a ruido.
\item Efectivo si hay muchas muestras de entrenamiento.
\end{itemize}
Desventajas
\begin{itemize}
\item Cuando queremos predecir nuevos ejemplos, se hace todo el trabajo:
\begin{enumerate}
\item Se calculan las distancias de las nuevas observaciones a predecir con el conjunto de entrenamiento.
\item Se ordenan y escogen las $k$ observaciones más cercanas.
\item Se predice la nueva observación como la clase mayoritaria en dichas observaciones.
\end{enumerate}
\item Hay que elegir el valor de $k$ y una distancia.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Predicción y efecto de $k$}
\begin{center}
\includegraphics[height=0.68\textheight]{knn_class.png}
\vfill
{\footnotesize Línea continua $k=3$, línea discontinua $k=5$. \href{https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm}{Fuente}}
\end{center}
\end{frame}

\begin{frame}[allowframebreaks, plain]
\centering
\includegraphics[width=0.8\textwidth]{knn1.pdf}

\break

\begin{center}
\includegraphics[width=0.8\textwidth]{knn15.pdf}
\end{center}

\break

\begin{center}
\includegraphics[width=0.8\textwidth]{knn50.pdf}
\end{center}
\end{frame}

\begin{frame}
\frametitle{Árboles de decisión}

\begin{itemize}
\item Crea modelos de regresión o clasificación en forma de árbol.
\item Para construir un árbol, se utiliza el siguiente algoritmo:
\begin{enumerate}
\item Elegir la variable que más información proporciona sobre la respuesta.
\item Particionar el conjunto de datos, de acuerdo a un valor de la variable anterior.
\item Para cada uno de los dos subconjuntos, repetir el procedimiento.
\item Paramos cuando ya nos queda una única observación o según un criterio de parada.
\end{enumerate}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Árboles de decisión: ejemplo}
\centering
\includegraphics[height=0.9\textheight]{arbol.pdf}
\end{frame}

\begin{frame}
\frametitle{Arboles de decisión: interpretabilidad}

\begin{itemize}
\item Hasta ahora solo hablamos del error de como medida de calidad de un modelo.
\item Sin embargo, a veces es también muy importante que el modelo sea fácil de entender e interpretar.
\item Esta es una de las principales ventajas de los árboles de decisión sobre el resto de modelos.
\item Otras ventajas son:
\begin{itemize}
\item Es capaz de trabajar con variables categóricas sin necesidad de codificación.
\item Rápido de crear, incluso con muchas observaciones y variables.
\end{itemize}
\item La principal desventaja es que es necesario controlar el crecimiento del árbol, ya que son muy propensos al sobreajuste.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Árboles de decisión vs regresión logística}
\centering
\includegraphics[height=0.8\textheight]{arbol_logreg.png}
\end{frame}

\begin{frame}
\frametitle{Métodos de conjunto: \textit{bagging}}

\begin{itemize}
\item La idea de combinar varios modelos es muy natural.
\item Queremos combinar clasificadores simples y además que tengan diversidad, es decir, que no aprendan todos lo mismo.
\item La diversidad la podemos introducir entrenando cada clasificador con una pequeña variación del conjunto de entrenamiento.
\item Dado un conjunto de entrenamiento, \textit{bagging} crea $m$ nuevos del mismo tamaño mediante muestreo uniforme con reemplazamiento.
\item Se puede ver que cada uno de estos $m$ conjuntos tiene un 63\% de muestras no repetidas.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Métodos de conjunto: ejemplo}

\centering
\includegraphics[height=0.85\textheight]{Ozone.png}
\end{frame}

\begin{frame}
\frametitle{Random Forest}

\begin{itemize}
\item Random Forest combina los árboles de decisión con la técnica de \texttt{bagging} para generar un conjunto de modelos.
\item Para cada uno de las particiones de \textit{bagging}, se entrena un árbol de decisión.
\item Finalmente se combinan las predicciones de los árboles, por ejemplo con el voto mayoritario.
\item Con respecto a los árboles de decisión, estos conjuntos:
\begin{itemize}
\item Pierden la capacidad interpretativa, aunque son capaces de dar una medida de lo importantes que son las variables.
\item Mejoran notablemente el error de predicción.
\item Solucionan el problema de sobreajuste.
\item Escogen un subconjunto aleatorio de variables en cada partición, para incrementar todavía más la diversidad.
\end{itemize}
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Redes neuronales}
\centering
\includegraphics[width=0.85\textwidth]{nn.png}
\end{frame}

\begin{frame}
\frametitle{Redes neuronales (cont.)}

\begin{itemize}
\item Inspiradas en el funcionamiento de los sistemas nerviosos biológicos.
\item Están compuestas de distintas capas, que a su vez están compuestas de neuronas. 
\item Las neuronas toman unos datos de entrada, los agregan y les aplican una función no lineal, devolviendo una única salida.
\item Se pueden usar para problemas de regresión y de clasificación modificando ligeramente la capa de salida.
\item Muy flexibles, se pueden añadir capas si la relación a aproximar es muy compleja.
\item Cuantas mas capas son necesarios más datos para ajustarlas correctamente.
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Representación}
\begin{itemize}
\item Las redes neuronales se pueden ver como un modelo en dos pasos:
\begin{enumerate}
\item Las capas intermedias crean una nueva ``representación'' de las variables de entrada.
\item La capa de salida ajusta un modelo líneal simple (regresión lineal, regresión logística). 
\end{enumerate}
\item Esta representación interna puede ser útil para entender nuestros datos.
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Ejemplo word2vec}
\begin{wideitemize}
\item Red neuronal que se entrena con grupos de $n$ palabras con el objetivo de intentar predecir la palabra del medio: \\ \textit{quick brown} \textbf{fox} \textit{jumps over}
\item Podemos proyectar el resultado de las capas intermedias en 2D.

\item Se ve como la distancia entre palabras representa su parecido en cuanto a significado semántico.

\item Dicha representación es útil para otro tipo de tareas relacionadas con procesamiento de texto: análisis de sentimiento, categorización, etc.
\end{wideitemize}
\end{frame}

\begin{frame}[plain]
\centering
\includegraphics[scale=0.4]{word2vec.png}
\end{frame}


\section{Análisis de resultados}

\begin{frame}
\frametitle{Diseño iterativo}

\begin{wideitemize}
\item Si el resultado del modelo no es tan bueno como nos gustaría no hay que perder la esperanza, ya es muy común que el análisis se realize de forma iterativa.
\item Podemos probar varias cosas:
\begin{enumerate}
\item Ajustar mejor el modelo: muchos de los modelos que hemos visto tienen parámetros que influyen mucho en el rendimiento (más a continuación).
\item Probar otros modelos: una opción muy común al hacer el análisis de unos datos es comenzar con modelos más simples e ir moviéndonos hacia modelos más complejos.
\item Obtener más datos: no siempre es posible, pero en general con más datos se consigue mejor resultado que un algoritmo más inteligente.
\end{enumerate}
\end{wideitemize}
\end{frame}

\begin{frame}
\frametitle{Conjunto de validación}
\begin{itemize}
\item Para escoger los valores de los parámetros de un modelo, podemos mirar el error de entrenamiento para varios valores y escoger el de menor error.
\item Al hacer esto, hemos visto que el error de entrenamiento es una medida sesgada y por tanto los valores de los parámetros pueden no funcionar bien en el conjunto de test.
\item Por tanto, necesitamos un tercer conjunto llamado conjunto de \textbf{validación} donde se va a medir el error para cada valor de los parámetros.
\item Finalmente, escogemos el que tenga menor error de validación.
\item El conjunto de test sigue sin tocar, y se usa como antes para estimar el error de generalización.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Validación cruzada}
\begin{itemize}
\item Si no tenemos muchos datos y no queremos hacer un conjunto de validación, podemos usar validación cruzada.
\end{itemize}

\begin{center}
\includegraphics[width=1\textwidth]{crossval.jpg}

{\footnotesize \href{https://commons.wikimedia.org/w/index.php?curid=17616792}{Fuente}}
\end{center}
\end{frame}

\begin{frame}
\frametitle{Selección de variables y reducción de dimensión}

\begin{itemize}
\item Una forma de reducir el tiempo de computación de los algoritmos de aprendizaje es reduciendo el número de variables.
\item Esto se puede hacer de dos maneras:
\begin{enumerate}
\item Seleccionando únicamente un subconjunto de las variables de acuerdo a algún criterio de relevancia para predecir la respuesta.
\item Proyectando nuestros datos a un espacio de menor dimensión.
\end{enumerate}
\item La primera opción tiene la ventaja de que las variables siguen siendo interpretables, es decir, tienen significado semántico.
\item El método más común de proyección es PCA o análisis de componentes principales.
\end{itemize}
\end{frame}

\section{Datos de gran tamaño}

\begin{frame}
\frametitle{Datos a gran escala}

\begin{itemize}
\item Hemos visto que la mayoría de los conjuntos de datos se pueden tratar en un portátil o servidor de gama media--alta.
\item Sin embargo en ocasiones tenemos conjuntos de datos que:
\begin{enumerate}
\item directamente no se pueden cargar en memoria
\item se pueden cargar pero el proceso de aprendizaje es prohibitivamente lento.
\end{enumerate}
\item Lo primero es preguntarse si de verdad hacen falta tantos datos, ya que no todos los problemas de aprendizaje son suficientemente complejos para requerir millones de ejemplos.
\item Ejemplo: clasificar imágenes de dígitos vs clasificar imágenes de objetos.
\item Si lo anterior falla, tenemos que recurrir a técnicas y herramientas de computación de alto rendimiento.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Paralelización}

\begin{itemize}
\item Se refiere a ejecutar un proceso a la vez en múltiples procesadores.
\item No siempre es posible paralelizar un algoritmo, al menos no de forma fácil.
\item Existen dos tipos principales que nos interesan:
\begin{itemize}
\item A nivel de operaciones: paraleliza la ejecución de las operaciones algebraicas.
\item A nivel de datos: nuestro problema se puede dividir en tareas independientes, que se pueden ejecutar en varios procesadores a la vez.
\end{itemize}
\item En cualquier caso esto no soluciona el problema de la memoria RAM.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Computación distribuida}

\begin{itemize}
\item Si tenemos disponible un clúster de servidores, podemos intentar aprovecharlos todos para hacer nuestros cálculos.
\item Esto es trivial si, por ejemplo:
\begin{enumerate}
\item Estamos buscando parámetros óptimos.
\item Estamos haciendo grupos de modelos, como \texttt{randomForest}.
\item Estamos ajustando un modelo para distintas particiones de entrenamiento y test.
\end{enumerate}
\item En otro caso, tenemos que usar herramientas que faciliten el trabajo como Hadoop o Spark.
\item Hadoop consiste principalmente en un sistema de ficheros distribuido, es decir, los datos se dividen en bloques y se reparten en los nodos, que es donde se realiza el cálculo.
\end{itemize}
\end{frame}

\end{document}
